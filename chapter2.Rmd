
# Chapter 2. Regression and model validation

This week I am doing regression analysis and model validation. But first I had to wrangle with the original data. That was pretty difficult for me, but it was good practice and I just need to practice a lot more 

## Description of the dataset

```{r}
# After I had been wrangling with the data and managed to write it out in a folder. I could use my own wrangled data for the further analysis... 

students2014 <- read.csv(file="~/Documents/GitHub/IODS-project/Data/learning2014.csv", header = TRUE, sep=",")

#Looking the data structure

str(students2014)
head(students2014)

```
This is a data of 166 observations (students). Of them we have 7 variables: the information about their gender, age, global attitude toward statistics (Attitude), exam points (Points) and their points related to different aspects of learning (Deep, strategic and surface learning). 



## A graphical overview of the data and show summaries of the variables in the data. 



```{r}
pairs(students2014[-1], col=students2014$gender)
summary(students2014)

```


I think this method is not very good. It??s very difficult to interpret it like this. I need a better graph..


```{r}
#use the packages GGally and ggplot2 and get some help with the graphical overview

library(GGally)
library(ggplot2)

p <- ggpairs(students2014, mapping = aes(alpha=0.5), lower=list(combo =wrap("facethist", bins=20)))

p

```

**This is better view of the data. All the variables (expect for the age, where skewness >0) are pretty nicely normally distributed.** This also tells me the correlation between the different variables. There doesn??t seem to be very strong correlation between the different variables since the correlation coefficients are between -0.3 - 0.4


## Making a regression model

```{r}
#Choose three variables as explanatory variables and fit a regression model where exam points is the target (dependent) variable.

model <- lm(Points ~ gender + Attitude + stra, data=students2014)
summary(model)



```

I chose to study the association of exam points (target value) with gender, attitude and strategic learning (explanatory variables). **Here I can see that the Attitude is the only one of these three explanatory values to be statistifically siginificant (p-value is < 0.05.). Also the t value 5.893 tells about the significance**

Next I make a regression model with **only the one significant explanatory variable "Attitude".** 

```{r}
model2 <- lm(Points ~ Attitude, data=students2014)
summary(model2)

```
Here I get the answer that the "Attitude" estimate is 0.35 and the p-value stays low (below 0.05 meaning it is significant). In other words this means that when Attitude increases by 1 unit the exam points increases by 0.35.

Next I need to explain and interpret the **multiple R squared of the model**.
R-squared is a statistical measure of how close the data are to the fitted regression line. Meaning how well the model fits my data.

**The definition of R-squared (selitysaste) is the percentage of the response variable variation that is explained by a linear model.**

R-squared = Explained variation / Total variation
R-squared is always between 0 and 100%:

Here in the summary *I can see the Multiple R-squared is 0.1906 -> 19% so this means that my model would explain only one fifth of the exam points around their mean.*

"In general, **the higher the R-squared, the better the model fits my data.** R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots."

"R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data!"


##Diagnostic plots to explain the assumptions of the model

There are several assumptions of linear regression model. With the help of the following plots I can analyze the residuals of my model and see how well my linear regression model is working here or is there some serious problems with it.

```{r}
plot(model2, which= c(2,1,5))
```

#### Residuals vs fitted plot (homoscedasticity)
This plot shows that the errors/residuals have constant variance. I can find a a equally spread residuals around a horizontal line without distinct patterns. This is a good indication!


#### Q-Q plot (normality)
**With the Q-Q plot** I can explore that the residuals are normally distributed. Here I can see that point are very close to the line expect of upper and lower tails where there is some deviation. However I think this still is reasonable and I could interpret that the errors are normally distributed.

#### Residuals vs Leverage 
This plot helps me to clear if I have outliers in my data that are influencial in my linear regression model. In my analysis I have no such cases that would be influencial in my model. All my cases are inside the Cook??s distance lines (I can not even see them)

